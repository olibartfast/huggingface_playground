{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80725b58",
   "metadata": {},
   "source": [
    "# VitPose: Vision Transformer for Human Pose Estimation\n",
    "\n",
    "This notebook demonstrates how to use VitPose (Vision Transformer for Pose Estimation) following the official Hugging Face documentation. VitPose is a **top-down** pose estimation model that requires two stages:\n",
    "\n",
    "1. **Object Detection**: First detect people in the image using RT-DETR\n",
    "2. **Pose Estimation**: Extract keypoints from detected person regions using VitPose\n",
    "\n",
    "## Key Features:\n",
    "- State-of-the-art pose estimation using Vision Transformers\n",
    "- Support for multiple model sizes (base, large, huge)\n",
    "- COCO keypoint format with 17 body landmarks\n",
    "- Batch processing and memory optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ea5250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Required Dependencies\n",
    "!pip install transformers torch torchvision\n",
    "!pip install Pillow matplotlib requests numpy\n",
    "!pip install supervision  # Optional: for advanced visualization\n",
    "!pip install opencv-python  # For alternative visualization options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d734fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import requests\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from PIL import Image, ImageDraw\n",
    "from typing import List, Dict, Tuple, Any\n",
    "\n",
    "# Import Transformers components for VitPose and RT-DETR\n",
    "from transformers import (\n",
    "    AutoProcessor, \n",
    "    RTDetrForObjectDetection, \n",
    "    VitPoseForPoseEstimation\n",
    ")\n",
    "\n",
    "# Set device for computation\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a54791e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load VitPose Model and Processor\n",
    "# Available models: usyd-community/vitpose-base-simple, vitpose-large-simple, vitpose-huge-simple\n",
    "\n",
    "model_name = \"usyd-community/vitpose-base-simple\"\n",
    "print(f\"Loading VitPose model: {model_name}\")\n",
    "\n",
    "# Load the VitPose processor and model\n",
    "vitpose_processor = AutoProcessor.from_pretrained(model_name)\n",
    "vitpose_model = VitPoseForPoseEstimation.from_pretrained(model_name, device_map=device)\n",
    "\n",
    "print(f\"‚úì VitPose model loaded successfully!\")\n",
    "print(f\"Model config - Hidden size: {vitpose_model.config.backbone_config.hidden_size}\")\n",
    "if hasattr(vitpose_model.config, 'num_keypoints'):\n",
    "    print(f\"Number of keypoints: {vitpose_model.config.num_keypoints}\")\n",
    "\n",
    "# For ViTPose++ models with Mixture of Experts, you can specify dataset_index\n",
    "# Available dataset indices:\n",
    "# 0: COCO validation 2017 dataset\n",
    "# 1: AiC dataset  \n",
    "# 2: MPII dataset\n",
    "# 3: AP-10K dataset\n",
    "# 4: APT-36K dataset\n",
    "# 5: COCO-WholeBody dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68096a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and Preprocess Input Image\n",
    "# Using the same image as in the official documentation\n",
    "\n",
    "url = \"http://images.cocodataset.org/val2017/000000000139.jpg\"\n",
    "print(f\"Loading image from: {url}\")\n",
    "\n",
    "# Download and load the image\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "print(f\"‚úì Image loaded successfully! Size: {image.size}\")\n",
    "\n",
    "# Display the original image\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(image)\n",
    "plt.title(\"Original Image\")\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# Alternative: Load local image\n",
    "# image = Image.open(\"path/to/your/image.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298a953b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 1: Person Detection using RT-DETR\n",
    "# VitPose is a top-down model that requires person bounding boxes\n",
    "\n",
    "print(\"Stage 1: Detecting people in the image...\")\n",
    "\n",
    "# Load RT-DETR model for object detection\n",
    "person_detector_name = \"PekingU/rtdetr_r50vd_coco_o365\"\n",
    "person_processor = AutoProcessor.from_pretrained(person_detector_name)\n",
    "person_model = RTDetrForObjectDetection.from_pretrained(person_detector_name, device_map=device)\n",
    "\n",
    "print(f\"‚úì RT-DETR model loaded: {person_detector_name}\")\n",
    "\n",
    "# Process image for object detection\n",
    "detection_inputs = person_processor(images=image, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Run object detection\n",
    "with torch.no_grad():\n",
    "    detection_outputs = person_model(**detection_inputs)\n",
    "\n",
    "# Post-process detection results\n",
    "detection_results = person_processor.post_process_object_detection(\n",
    "    detection_outputs, \n",
    "    target_sizes=torch.tensor([(image.height, image.width)]), \n",
    "    threshold=0.3\n",
    ")\n",
    "result = detection_results[0]  # Results for first (and only) image\n",
    "\n",
    "# Filter for person detections (label 0 in COCO dataset)\n",
    "person_boxes = result[\"boxes\"][result[\"labels\"] == 0]\n",
    "person_scores = result[\"scores\"][result[\"labels\"] == 0]\n",
    "\n",
    "print(f\"‚úì Detected {len(person_boxes)} person(s) in the image\")\n",
    "\n",
    "# Convert boxes from VOC format (x1, y1, x2, y2) to COCO format (x, y, w, h)\n",
    "person_boxes_coco = person_boxes.cpu().numpy().copy()\n",
    "person_boxes_coco[:, 2] = person_boxes_coco[:, 2] - person_boxes_coco[:, 0]  # width\n",
    "person_boxes_coco[:, 3] = person_boxes_coco[:, 3] - person_boxes_coco[:, 1]  # height\n",
    "\n",
    "print(f\"Person boxes (COCO format): {person_boxes_coco}\")\n",
    "print(f\"Detection scores: {person_scores.cpu().numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6c3d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 2: Pose Estimation using VitPose\n",
    "# Process detected person regions through VitPose\n",
    "\n",
    "if len(person_boxes_coco) == 0:\n",
    "    print(\"‚ùå No people detected. Cannot proceed with pose estimation.\")\n",
    "else:\n",
    "    print(f\"Stage 2: Running pose estimation on {len(person_boxes_coco)} detected person(s)...\")\n",
    "    \n",
    "    # Prepare inputs for VitPose - IMPORTANT: boxes parameter is required!\n",
    "    pose_inputs = vitpose_processor(\n",
    "        image, \n",
    "        boxes=[person_boxes_coco],  # Pass bounding boxes as required\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "    \n",
    "    print(f\"Pose input shape: {pose_inputs.pixel_values.shape}\")\n",
    "    \n",
    "    # Run pose estimation\n",
    "    with torch.no_grad():\n",
    "        pose_outputs = vitpose_model(**pose_inputs)\n",
    "    \n",
    "    print(f\"Raw heatmaps shape: {pose_outputs.heatmaps.shape}\")\n",
    "    \n",
    "    # Post-process pose estimation results\n",
    "    pose_results = vitpose_processor.post_process_pose_estimation(\n",
    "        pose_outputs, \n",
    "        boxes=[person_boxes_coco]\n",
    "    )\n",
    "    \n",
    "    # Extract results for the first image\n",
    "    image_pose_results = pose_results[0]\n",
    "    \n",
    "    print(f\"‚úì Pose estimation completed!\")\n",
    "    print(f\"‚úì Detected poses for {len(image_pose_results)} person(s)\")\n",
    "    \n",
    "    # Display pose information\n",
    "    for i, pose_result in enumerate(image_pose_results):\n",
    "        keypoints = pose_result['keypoints']\n",
    "        scores = pose_result['scores']\n",
    "        print(f\"\\nPerson {i+1}:\")\n",
    "        print(f\"  Keypoints shape: {keypoints.shape}\")\n",
    "        print(f\"  Scores shape: {scores.shape}\")\n",
    "        print(f\"  Average confidence: {scores.mean():.3f}\")\n",
    "        print(f\"  High confidence keypoints: {(scores > 0.5).sum()}/{len(scores)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020d452d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Keypoints and Skeleton\n",
    "# Following the official documentation visualization approach\n",
    "\n",
    "if len(person_boxes_coco) > 0:\n",
    "    # COCO keypoint names for reference\n",
    "    COCO_KEYPOINT_NAMES = [\n",
    "        'nose', 'left_eye', 'right_eye', 'left_ear', 'right_ear',\n",
    "        'left_shoulder', 'right_shoulder', 'left_elbow', 'right_elbow', \n",
    "        'left_wrist', 'right_wrist', 'left_hip', 'right_hip',\n",
    "        'left_knee', 'right_knee', 'left_ankle', 'right_ankle'\n",
    "    ]\n",
    "    \n",
    "    # Color palette from the official documentation\n",
    "    palette = np.array([\n",
    "        [255, 128, 0], [255, 153, 51], [255, 178, 102], [230, 230, 0],\n",
    "        [255, 153, 255], [153, 204, 255], [255, 102, 255], [255, 51, 255],\n",
    "        [102, 178, 255], [51, 153, 255], [255, 153, 153], [255, 102, 102],\n",
    "        [255, 51, 51], [153, 255, 153], [102, 255, 102], [51, 255, 51],\n",
    "        [0, 255, 0], [0, 0, 255], [255, 0, 0], [255, 255, 255]\n",
    "    ])\n",
    "    \n",
    "    # Link colors and keypoint colors from official docs\n",
    "    link_colors = palette[[0, 0, 0, 0, 7, 7, 7, 9, 9, 9, 9, 9, 16, 16, 16, 16, 16, 16, 16]]\n",
    "    keypoint_colors = palette[[16, 16, 16, 16, 16, 9, 9, 9, 9, 9, 9, 0, 0, 0, 0, 0, 0]]\n",
    "    \n",
    "    # Get keypoint edges from model config\n",
    "    keypoint_edges = vitpose_model.config.edges\n",
    "    \n",
    "    # Create visualization\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Original image\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(image)\n",
    "    plt.title(\"Original Image\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Image with pose overlay\n",
    "    plt.subplot(1, 2, 2)\n",
    "    numpy_image = np.array(image.copy())\n",
    "    \n",
    "    # Draw poses for each detected person\n",
    "    for pose_result in image_pose_results:\n",
    "        keypoints = np.array(pose_result[\"keypoints\"])\n",
    "        scores = np.array(pose_result[\"scores\"])\n",
    "        \n",
    "        # Draw keypoint connections (skeleton)\n",
    "        if keypoint_edges is not None:\n",
    "            for sk_id, (joint1, joint2) in enumerate(keypoint_edges):\n",
    "                if joint1 < len(keypoints) and joint2 < len(keypoints):\n",
    "                    x1, y1, score1 = keypoints[joint1][0], keypoints[joint1][1], scores[joint1]\n",
    "                    x2, y2, score2 = keypoints[joint2][0], keypoints[joint2][1], scores[joint2]\n",
    "                    \n",
    "                    # Only draw if both keypoints are confident enough\n",
    "                    if score1 > 0.3 and score2 > 0.3:\n",
    "                        x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n",
    "                        if (0 <= x1 < numpy_image.shape[1] and 0 <= y1 < numpy_image.shape[0] and\n",
    "                            0 <= x2 < numpy_image.shape[1] and 0 <= y2 < numpy_image.shape[0]):\n",
    "                            \n",
    "                            color = tuple(int(c) for c in link_colors[sk_id % len(link_colors)])\n",
    "                            cv2.line(numpy_image, (x1, y1), (x2, y2), color, thickness=2)\n",
    "        \n",
    "        # Draw keypoints\n",
    "        for kid, (kpt, kpt_score) in enumerate(zip(keypoints, scores)):\n",
    "            x_coord, y_coord = int(kpt[0]), int(kpt[1])\n",
    "            if kpt_score > 0.3:  # Only draw confident keypoints\n",
    "                if (0 <= x_coord < numpy_image.shape[1] and 0 <= y_coord < numpy_image.shape[0]):\n",
    "                    color = tuple(int(c) for c in keypoint_colors[kid % len(keypoint_colors)])\n",
    "                    cv2.circle(numpy_image, (x_coord, y_coord), 4, color, -1)\n",
    "                    # Add white border\n",
    "                    cv2.circle(numpy_image, (x_coord, y_coord), 4, (255, 255, 255), 1)\n",
    "    \n",
    "    plt.imshow(numpy_image)\n",
    "    plt.title(f\"Detected Poses ({len(image_pose_results)} person(s))\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"‚úì Pose visualization completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362dc445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import cv2 for drawing functions (needed for visualization)\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e26af84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Results to JSON Format\n",
    "\n",
    "if len(person_boxes_coco) > 0:\n",
    "    # Prepare data for JSON export\n",
    "    results_data = {\n",
    "        \"image_info\": {\n",
    "            \"width\": image.width,\n",
    "            \"height\": image.height,\n",
    "            \"url\": url\n",
    "        },\n",
    "        \"detection_info\": {\n",
    "            \"num_persons_detected\": len(person_boxes_coco),\n",
    "            \"detection_threshold\": 0.3,\n",
    "            \"person_boxes\": person_boxes_coco.tolist(),\n",
    "            \"detection_scores\": person_scores.cpu().numpy().tolist()\n",
    "        },\n",
    "        \"pose_results\": []\n",
    "    }\n",
    "    \n",
    "    # Add pose data for each detected person\n",
    "    for i, pose_result in enumerate(image_pose_results):\n",
    "        keypoints = pose_result['keypoints'].numpy()\n",
    "        scores = pose_result['scores'].numpy()\n",
    "        \n",
    "        pose_data = {\n",
    "            \"person_id\": i,\n",
    "            \"bbox\": person_boxes_coco[i].tolist(),\n",
    "            \"detection_score\": float(person_scores[i].cpu().numpy()),\n",
    "            \"keypoints\": [],\n",
    "            \"average_keypoint_confidence\": float(scores.mean()),\n",
    "            \"high_confidence_keypoints\": int((scores > 0.5).sum())\n",
    "        }\n",
    "        \n",
    "        # Add individual keypoint data\n",
    "        for j, (kpt, score) in enumerate(zip(keypoints, scores)):\n",
    "            keypoint_data = {\n",
    "                \"id\": j,\n",
    "                \"name\": COCO_KEYPOINT_NAMES[j] if j < len(COCO_KEYPOINT_NAMES) else f\"keypoint_{j}\",\n",
    "                \"x\": float(kpt[0]),\n",
    "                \"y\": float(kpt[1]),\n",
    "                \"confidence\": float(score),\n",
    "                \"visible\": bool(score > 0.3)\n",
    "            }\n",
    "            pose_data[\"keypoints\"].append(keypoint_data)\n",
    "        \n",
    "        results_data[\"pose_results\"].append(pose_data)\n",
    "    \n",
    "    # Save to JSON file\n",
    "    output_file = \"vitpose_results.json\"\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(results_data, f, indent=2)\n",
    "    \n",
    "    print(f\"‚úì Results saved to {output_file}\")\n",
    "    print(f\"‚úì Summary: {len(image_pose_results)} poses detected\")\n",
    "    print(f\"‚úì Total keypoints: {sum(len(pose['keypoints']) for pose in results_data['pose_results'])}\")\n",
    "    \n",
    "    # Display a sample of the JSON structure\n",
    "    print(\"\\nSample JSON structure:\")\n",
    "    sample_person = results_data[\"pose_results\"][0] if results_data[\"pose_results\"] else {}\n",
    "    if sample_person:\n",
    "        print(f\"Person 1 - Avg confidence: {sample_person['average_keypoint_confidence']:.3f}\")\n",
    "        print(f\"Sample keypoints (first 3):\")\n",
    "        for kpt in sample_person[\"keypoints\"][:3]:\n",
    "            print(f\"  {kpt['name']}: ({kpt['x']:.1f}, {kpt['y']:.1f}) conf={kpt['confidence']:.3f}\")\n",
    "else:\n",
    "    print(\"‚ùå No poses to save - no people were detected in the image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743fcec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch Processing Multiple Images\n",
    "\n",
    "def process_multiple_images(image_urls: List[str], confidence_threshold: float = 0.3) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Process multiple images for pose estimation in batch.\n",
    "    \n",
    "    Args:\n",
    "        image_urls: List of image URLs or file paths\n",
    "        confidence_threshold: Threshold for person detection\n",
    "    \n",
    "    Returns:\n",
    "        List of results for each image\n",
    "    \"\"\"\n",
    "    all_results = []\n",
    "    \n",
    "    for i, img_url in enumerate(image_urls):\n",
    "        print(f\"\\n--- Processing Image {i+1}/{len(image_urls)} ---\")\n",
    "        \n",
    "        try:\n",
    "            # Load image\n",
    "            if img_url.startswith('http'):\n",
    "                img = Image.open(requests.get(img_url, stream=True).raw)\n",
    "            else:\n",
    "                img = Image.open(img_url)\n",
    "            \n",
    "            print(f\"Loaded image: {img.size}\")\n",
    "            \n",
    "            # Stage 1: Person Detection\n",
    "            detection_inputs = person_processor(images=img, return_tensors=\"pt\").to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                detection_outputs = person_model(**detection_inputs)\n",
    "            \n",
    "            detection_results = person_processor.post_process_object_detection(\n",
    "                detection_outputs, \n",
    "                target_sizes=torch.tensor([(img.height, img.width)]), \n",
    "                threshold=confidence_threshold\n",
    "            )\n",
    "            \n",
    "            result = detection_results[0]\n",
    "            person_boxes = result[\"boxes\"][result[\"labels\"] == 0]\n",
    "            person_scores = result[\"scores\"][result[\"labels\"] == 0]\n",
    "            \n",
    "            if len(person_boxes) == 0:\n",
    "                print(\"No people detected in this image\")\n",
    "                all_results.append({\n",
    "                    \"image_index\": i,\n",
    "                    \"image_url\": img_url,\n",
    "                    \"persons_detected\": 0,\n",
    "                    \"poses\": []\n",
    "                })\n",
    "                continue\n",
    "                \n",
    "            # Convert to COCO format\n",
    "            person_boxes_coco = person_boxes.cpu().numpy().copy()\n",
    "            person_boxes_coco[:, 2] = person_boxes_coco[:, 2] - person_boxes_coco[:, 0]\n",
    "            person_boxes_coco[:, 3] = person_boxes_coco[:, 3] - person_boxes_coco[:, 1]\n",
    "            \n",
    "            # Stage 2: Pose Estimation\n",
    "            pose_inputs = vitpose_processor(\n",
    "                img, \n",
    "                boxes=[person_boxes_coco], \n",
    "                return_tensors=\"pt\"\n",
    "            ).to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                pose_outputs = vitpose_model(**pose_inputs)\n",
    "            \n",
    "            pose_results = vitpose_processor.post_process_pose_estimation(\n",
    "                pose_outputs, \n",
    "                boxes=[person_boxes_coco]\n",
    "            )\n",
    "            \n",
    "            # Store results\n",
    "            image_results = {\n",
    "                \"image_index\": i,\n",
    "                \"image_url\": img_url,\n",
    "                \"image_size\": img.size,\n",
    "                \"persons_detected\": len(person_boxes_coco),\n",
    "                \"poses\": []\n",
    "            }\n",
    "            \n",
    "            for j, pose_result in enumerate(pose_results[0]):\n",
    "                keypoints = pose_result['keypoints'].numpy()\n",
    "                scores = pose_result['scores'].numpy()\n",
    "                \n",
    "                image_results[\"poses\"].append({\n",
    "                    \"person_id\": j,\n",
    "                    \"bbox\": person_boxes_coco[j].tolist(),\n",
    "                    \"detection_confidence\": float(person_scores[j].cpu().numpy()),\n",
    "                    \"avg_keypoint_confidence\": float(scores.mean()),\n",
    "                    \"visible_keypoints\": int((scores > 0.3).sum()),\n",
    "                    \"keypoints\": keypoints.tolist(),\n",
    "                    \"scores\": scores.tolist()\n",
    "                })\n",
    "            \n",
    "            all_results.append(image_results)\n",
    "            print(f\"‚úì Processed: {len(person_boxes_coco)} person(s) detected\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error processing image {i+1}: {e}\")\n",
    "            all_results.append({\n",
    "                \"image_index\": i,\n",
    "                \"image_url\": img_url,\n",
    "                \"error\": str(e),\n",
    "                \"persons_detected\": 0,\n",
    "                \"poses\": []\n",
    "            })\n",
    "            \n",
    "        # Clear memory\n",
    "        torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "# Example: Process multiple images\n",
    "sample_urls = [\n",
    "    \"http://images.cocodataset.org/val2017/000000000139.jpg\",\n",
    "    \"http://images.cocodataset.org/val2017/000000039769.jpg\",\n",
    "    # Add more URLs as needed\n",
    "]\n",
    "\n",
    "print(\"Demo: Batch processing multiple images...\")\n",
    "batch_results = process_multiple_images(sample_urls[:1])  # Process just 1 for demo\n",
    "\n",
    "# Summary\n",
    "print(\"\\n=== Batch Processing Summary ===\")\n",
    "total_images = len(batch_results)\n",
    "total_persons = sum(result.get(\"persons_detected\", 0) for result in batch_results)\n",
    "successful_images = len([r for r in batch_results if \"error\" not in r])\n",
    "\n",
    "print(f\"Images processed: {total_images}\")\n",
    "print(f\"Successful: {successful_images}\")\n",
    "print(f\"Total persons detected: {total_persons}\")\n",
    "print(f\"Average persons per image: {total_persons/max(successful_images, 1):.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634fb134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance Optimization and Memory Management\n",
    "\n",
    "# 1. Memory cleanup function\n",
    "def cleanup_gpu_memory():\n",
    "    \"\"\"Clean up GPU memory to prevent memory leaks.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        print(\"‚úì GPU memory cache cleared\")\n",
    "    \n",
    "    import gc\n",
    "    gc.collect()\n",
    "    print(\"‚úì Garbage collection completed\")\n",
    "\n",
    "# 2. Quantization for memory efficiency (optional)\n",
    "def load_quantized_models():\n",
    "    \"\"\"Load models with quantization for reduced memory usage.\"\"\"\n",
    "    try:\n",
    "        from transformers import BitsAndBytesConfig\n",
    "        \n",
    "        # 4-bit quantization config\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\"\n",
    "        )\n",
    "        \n",
    "        # Load quantized models\n",
    "        quantized_vitpose = VitPoseForPoseEstimation.from_pretrained(\n",
    "            \"usyd-community/vitpose-base-simple\",\n",
    "            quantization_config=bnb_config,\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "        \n",
    "        print(\"‚úì Quantized VitPose model loaded\")\n",
    "        return quantized_vitpose\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"‚ö†Ô∏è BitsAndBytesConfig not available. Install with: pip install bitsandbytes\")\n",
    "        return None\n",
    "\n",
    "# 3. Performance monitoring\n",
    "def monitor_performance():\n",
    "    \"\"\"Monitor GPU memory usage and model performance.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"GPU Device: {torch.cuda.get_device_name()}\")\n",
    "        print(f\"Total GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "        print(f\"Allocated GPU memory: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "        print(f\"Cached GPU memory: {torch.cuda.memory_reserved() / 1e9:.2f} GB\")\n",
    "        print(f\"Free GPU memory: {(torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_reserved()) / 1e9:.2f} GB\")\n",
    "    else:\n",
    "        print(\"Using CPU - no GPU memory to monitor\")\n",
    "\n",
    "# 4. Model comparison function\n",
    "def compare_model_sizes():\n",
    "    \"\"\"Compare different VitPose model sizes.\"\"\"\n",
    "    models = [\n",
    "        \"usyd-community/vitpose-base-simple\",\n",
    "        \"usyd-community/vitpose-large-simple\", \n",
    "        \"usyd-community/vitpose-huge-simple\"\n",
    "    ]\n",
    "    \n",
    "    print(\"VitPose Model Comparison:\")\n",
    "    print(\"Model Size | Parameters | Performance | Memory Usage\")\n",
    "    print(\"-\" * 50)\n",
    "    print(\"Base       | ~100M      | Good        | Low\")\n",
    "    print(\"Large      | ~300M      | Better      | Medium\") \n",
    "    print(\"Huge       | ~600M      | Best        | High\")\n",
    "    print(\"\\nChoose based on your hardware capabilities and accuracy requirements.\")\n",
    "\n",
    "# Demo the optimization features\n",
    "print(\"=== Performance Optimization Demo ===\")\n",
    "\n",
    "# Monitor current performance\n",
    "print(\"\\n1. Current Memory Usage:\")\n",
    "monitor_performance()\n",
    "\n",
    "# Clean up memory\n",
    "print(\"\\n2. Cleaning up memory:\")\n",
    "cleanup_gpu_memory()\n",
    "monitor_performance()\n",
    "\n",
    "# Show model comparison\n",
    "print(\"\\n3. Model Size Comparison:\")\n",
    "compare_model_sizes()\n",
    "\n",
    "# Tips for optimization\n",
    "print(\"\\n=== Optimization Tips ===\")\n",
    "print(\"üöÄ Performance Tips:\")\n",
    "print(\"  ‚Ä¢ Use GPU when available (CUDA)\")\n",
    "print(\"  ‚Ä¢ Enable mixed precision with torch.bfloat16\")  \n",
    "print(\"  ‚Ä¢ Use quantization for memory-constrained environments\")\n",
    "print(\"  ‚Ä¢ Process images in batches when possible\")\n",
    "print(\"  ‚Ä¢ Clear GPU cache between processing sessions\")\n",
    "print(\"  ‚Ä¢ Choose appropriate model size for your hardware\")\n",
    "\n",
    "print(\"\\nüíæ Memory Management:\")\n",
    "print(\"  ‚Ä¢ Monitor GPU memory usage regularly\")\n",
    "print(\"  ‚Ä¢ Use torch.cuda.empty_cache() after processing\")\n",
    "print(\"  ‚Ä¢ Consider using CPU for very large batches\")\n",
    "print(\"  ‚Ä¢ Implement proper error handling and cleanup\")\n",
    "\n",
    "print(\"\\n‚ö° Speed Optimization:\")\n",
    "print(\"  ‚Ä¢ Use torch.no_grad() for inference\")\n",
    "print(\"  ‚Ä¢ Enable attention optimization with 'sdpa'\")\n",
    "print(\"  ‚Ä¢ Preload models to avoid repeated loading\")\n",
    "print(\"  ‚Ä¢ Use appropriate batch sizes for your GPU\")\n",
    "\n",
    "print(\"\\n‚úì Optimization setup completed!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
